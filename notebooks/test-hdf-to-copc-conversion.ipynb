{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete CALIPSO HDF4 to COPC Conversion Pipeline\n",
    "\n",
    "This notebook demonstrates the complete conversion pipeline:\n",
    "1. HDF4 → HDF5 (using h4toh5convert)\n",
    "2. HDF5 → Text (extracting 3D variables)\n",
    "3. Text → LAS (using PDAL)\n",
    "4. LAS → COPC (Cloud-Optimized Point Cloud)\n",
    "\n",
    "The final COPC format is optimized for streaming and cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from calipso_tool.converter import (\n",
    "    h4_to_h5, h5_to_txt, txt_to_las_pipeline, \n",
    "    las_to_copc_pipeline, h4_to_copc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup and File Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input file and variable\n",
    "hdf4_file = Path(\"CAL_LID_L3_Tropospheric_APro_AllSky-Standard-V4-20.2018-12D.hdf\")\n",
    "variable_name = \"var_to_grab\"  # UPDATE THIS with your actual variable name!\n",
    "\n",
    "# Check if file exists\n",
    "if hdf4_file.exists():\n",
    "    print(f\"✓ Input file found: {hdf4_file}\")\n",
    "    print(f\"  File size: {hdf4_file.stat().st_size / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"✗ Input file not found: {hdf4_file}\")\n",
    "    print(\"  Please ensure the HDF4 file is in the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: One-Step Complete Pipeline\n",
    "\n",
    "The easiest way - convert HDF4 directly to COPC with a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdf4_file.exists():\n",
    "    try:\n",
    "        # Complete pipeline in one call\n",
    "        copc_file, h5_file, txt_file, las_file = h4_to_copc(\n",
    "            input_h4=hdf4_file,\n",
    "            variable_name=variable_name,\n",
    "            altitude_units=\"km\",  # Will convert to meters\n",
    "            keep_intermediates=True  # Keep all intermediate files for inspection\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nAll files created:\")\n",
    "        if h5_file: print(f\"  HDF5: {h5_file} ({h5_file.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "        if txt_file: print(f\"  Text: {txt_file} ({txt_file.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "        if las_file: print(f\"  LAS: {las_file} ({las_file.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "        print(f\"  COPC: {copc_file} ({copc_file.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed: {e}\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"1. Make sure 'variable_name' matches a variable in your HDF5 file\")\n",
    "        print(\"2. Ensure PDAL is installed (conda install -c conda-forge pdal)\")\n",
    "        print(\"3. Check that the h4toh5convert binary is available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Step-by-Step Pipeline\n",
    "\n",
    "For more control and debugging, run each step separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: HDF4 to HDF5\n",
    "if hdf4_file.exists():\n",
    "    h5_file = hdf4_file.with_suffix('.h5')\n",
    "    \n",
    "    try:\n",
    "        print(\"Step 1: Converting HDF4 to HDF5...\")\n",
    "        h4_to_h5(hdf4_file, h5_file)\n",
    "        print(f\"✓ Created: {h5_file} ({h5_file.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ HDF4 to HDF5 conversion failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore HDF5 structure to find correct variable name\n",
    "if 'h5_file' in locals() and h5_file.exists():\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        print(\"Available variables in HDF5 file:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        def list_variables(name, obj):\n",
    "            if isinstance(obj, h5py.Dataset):\n",
    "                # Check if it's a 3D variable\n",
    "                if len(obj.shape) == 3:\n",
    "                    print(f\"✓ {name}: shape={obj.shape} (3D variable)\")\n",
    "                else:\n",
    "                    print(f\"  {name}: shape={obj.shape}\")\n",
    "        \n",
    "        f.visititems(list_variables)\n",
    "        \n",
    "        print(\"\\nNote: You need a 3D variable for point cloud conversion.\")\n",
    "        print(\"Update 'variable_name' with one of the 3D variables above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: HDF5 to Text\n",
    "if 'h5_file' in locals() and h5_file.exists():\n",
    "    txt_file = h5_file.with_suffix('.txt')\n",
    "    \n",
    "    try:\n",
    "        print(\"Step 2: Converting HDF5 to text...\")\n",
    "        h5_to_txt(h5_file, txt_file, variable_name, altitude_units=\"km\")\n",
    "        print(f\"✓ Created: {txt_file} ({txt_file.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "        \n",
    "        # Preview the text file\n",
    "        df_preview = pd.read_csv(txt_file, sep=' ', nrows=5)\n",
    "        print(f\"\\nPreview of text file:\")\n",
    "        print(df_preview)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ HDF5 to text conversion failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Text to LAS\n",
    "if 'txt_file' in locals() and txt_file.exists():\n",
    "    las_file = txt_file.with_suffix('.las')\n",
    "    \n",
    "    try:\n",
    "        print(\"Step 3: Converting text to LAS...\")\n",
    "        txt_to_las_pipeline(txt_file, las_file, variable_name)\n",
    "        print(f\"✓ Created: {las_file} ({las_file.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Text to LAS conversion failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: LAS to COPC\n",
    "if 'las_file' in locals() and las_file.exists():\n",
    "    copc_file = las_file.parent / f\"{las_file.stem}.copc.laz\"\n",
    "    \n",
    "    try:\n",
    "        print(\"Step 4: Converting LAS to COPC...\")\n",
    "        las_to_copc_pipeline(las_file, copc_file)\n",
    "        print(f\"✓ Created: {copc_file} ({copc_file.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "        \n",
    "        # Calculate compression ratio\n",
    "        las_size = las_file.stat().st_size / (1024*1024)\n",
    "        copc_size = copc_file.stat().st_size / (1024*1024)\n",
    "        compression = (1 - copc_size/las_size) * 100\n",
    "        print(f\"\\nCompression achieved: {compression:.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ LAS to COPC conversion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Final COPC File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed information about the COPC file\n",
    "copc_files = list(Path(\".\").glob(\"*.copc.laz\"))\n",
    "\n",
    "if copc_files:\n",
    "    copc_file = copc_files[0]\n",
    "    print(f\"Inspecting COPC file: {copc_file}\\n\")\n",
    "    \n",
    "    # Get PDAL info\n",
    "    result = subprocess.run(\n",
    "        [\"pdal\", \"info\", str(copc_file), \"--all\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        import json\n",
    "        info = json.loads(result.stdout)\n",
    "        \n",
    "        # Extract key information\n",
    "        if \"stats\" in info and \"statistic\" in info[\"stats\"]:\n",
    "            stats = info[\"stats\"][\"statistic\"][0]\n",
    "            print(f\"Point count: {stats.get('count', 'N/A'):,}\")\n",
    "            print(f\"\\nDimensions:\")\n",
    "            for dim in ['X', 'Y', 'Z', variable_name]:\n",
    "                if dim in stats:\n",
    "                    dim_stats = stats[dim]\n",
    "                    print(f\"  {dim}: [{dim_stats.get('minimum', 'N/A'):.3f}, {dim_stats.get('maximum', 'N/A'):.3f}]\")\n",
    "    else:\n",
    "        print(f\"Error getting COPC info: {result.stderr}\")\n",
    "else:\n",
    "    print(\"No COPC files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing\n",
    "\n",
    "Convert multiple CALIPSO files at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all HDF4 files in directory\n",
    "hdf4_files = list(Path(\".\").glob(\"*.hdf\"))\n",
    "\n",
    "if len(hdf4_files) > 1:\n",
    "    print(f\"Found {len(hdf4_files)} HDF4 files to process\\n\")\n",
    "    \n",
    "    successful = []\n",
    "    failed = []\n",
    "    \n",
    "    for hdf4_file in hdf4_files:\n",
    "        try:\n",
    "            print(f\"Processing: {hdf4_file.name}\")\n",
    "            copc_file, _, _, _ = h4_to_copc(\n",
    "                hdf4_file,\n",
    "                variable_name=variable_name,\n",
    "                keep_intermediates=False  # Clean up intermediate files\n",
    "            )\n",
    "            successful.append((hdf4_file, copc_file))\n",
    "            print(f\"✓ Success: {copc_file.name}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed.append((hdf4_file, str(e)))\n",
    "            print(f\"✗ Failed: {e}\\n\")\n",
    "    \n",
    "    print(f\"\\nBatch processing complete:\")\n",
    "    print(f\"  Successful: {len(successful)}\")\n",
    "    print(f\"  Failed: {len(failed)}\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\nFailed files:\")\n",
    "        for f, error in failed:\n",
    "            print(f\"  - {f.name}: {error}\")\n",
    "            \n",
    "elif len(hdf4_files) == 1:\n",
    "    print(\"Only one HDF4 file found. Use the methods above for single file processing.\")\n",
    "else:\n",
    "    print(\"No HDF4 files found in current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up intermediate files if needed\n",
    "def cleanup_intermediates(keep_copc=True):\n",
    "    \"\"\"\n",
    "    Remove intermediate files, optionally keeping COPC files.\n",
    "    \"\"\"\n",
    "    patterns = ['*.h5', '*.txt', '*.las']\n",
    "    if not keep_copc:\n",
    "        patterns.append('*.copc.laz')\n",
    "    \n",
    "    removed = []\n",
    "    for pattern in patterns:\n",
    "        for file in Path(\".\").glob(pattern):\n",
    "            file.unlink()\n",
    "            removed.append(file.name)\n",
    "    \n",
    "    if removed:\n",
    "        print(f\"Removed {len(removed)} files:\")\n",
    "        for f in removed:\n",
    "            print(f\"  - {f}\")\n",
    "    else:\n",
    "        print(\"No intermediate files to clean up.\")\n",
    "\n",
    "# Uncomment to run cleanup\n",
    "# cleanup_intermediates(keep_copc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "You've successfully converted CALIPSO HDF4 data to Cloud-Optimized Point Cloud format!\n",
    "\n",
    "**What you can do with COPC files:**\n",
    "1. **Visualize** in point cloud viewers (CloudCompare, QGIS, Potree)\n",
    "2. **Stream** efficiently from cloud storage (S3, Azure, GCS)\n",
    "3. **Process** with PDAL for filtering, classification, or analysis\n",
    "4. **Integrate** into GIS workflows\n",
    "\n",
    "**Command-line usage:**\n",
    "```bash\n",
    "# Convert single file\n",
    "python -m calipso_tool.converter input.hdf -v Extinction_Coefficient_532\n",
    "\n",
    "# Batch conversion\n",
    "for f in *.hdf; do\n",
    "    python -m calipso_tool.converter \"$f\" -v Temperature_Met\n",
    "done\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}